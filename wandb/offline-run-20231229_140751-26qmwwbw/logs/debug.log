2023-12-29 14:07:51,157 INFO    MainThread:2469618 [wandb_setup.py:_flush():76] Current SDK version is 0.16.1
2023-12-29 14:07:51,157 INFO    MainThread:2469618 [wandb_setup.py:_flush():76] Configure stats pid to 2469618
2023-12-29 14:07:51,157 INFO    MainThread:2469618 [wandb_setup.py:_flush():76] Loading settings from /home/eku/.config/wandb/settings
2023-12-29 14:07:51,157 INFO    MainThread:2469618 [wandb_setup.py:_flush():76] Loading settings from /home/eku/cpu-gan/wandb/settings
2023-12-29 14:07:51,157 INFO    MainThread:2469618 [wandb_setup.py:_flush():76] Loading settings from environment variables: {'disabled': 'True'}
2023-12-29 14:07:51,157 INFO    MainThread:2469618 [wandb_setup.py:_flush():76] Applying setup settings: {'_disable_service': False}
2023-12-29 14:07:51,157 INFO    MainThread:2469618 [wandb_setup.py:_flush():76] Inferring run settings from compute environment: {'program_relpath': 'train.py', 'program_abspath': '/home/eku/cpu-gan/train.py', 'program': 'train.py'}
2023-12-29 14:07:51,157 INFO    MainThread:2469618 [wandb_init.py:_log_setup():524] Logging user logs to /home/eku/cpu-gan/wandb/offline-run-20231229_140751-26qmwwbw/logs/debug.log
2023-12-29 14:07:51,157 INFO    MainThread:2469618 [wandb_init.py:_log_setup():525] Logging internal logs to /home/eku/cpu-gan/wandb/offline-run-20231229_140751-26qmwwbw/logs/debug-internal.log
2023-12-29 14:07:51,157 INFO    MainThread:2469618 [wandb_init.py:init():564] calling init triggers
2023-12-29 14:07:51,157 INFO    MainThread:2469618 [wandb_init.py:init():571] wandb.init called with sweep_config: {}
config: {}
2023-12-29 14:07:51,157 INFO    MainThread:2469618 [wandb_init.py:init():614] starting backend
2023-12-29 14:07:51,157 INFO    MainThread:2469618 [wandb_init.py:init():618] setting up manager
2023-12-29 14:07:51,158 INFO    MainThread:2469618 [backend.py:_multiprocessing_setup():105] multiprocessing start_methods=fork,spawn,forkserver, using: spawn
2023-12-29 14:07:51,160 INFO    MainThread:2469618 [wandb_init.py:init():624] backend started and connected
2023-12-29 14:07:51,164 INFO    MainThread:2469618 [wandb_init.py:init():716] updated telemetry
2023-12-29 14:07:51,196 INFO    MainThread:2469618 [wandb_init.py:init():749] communicating run to backend with 90.0 second timeout
2023-12-29 14:07:51,200 INFO    MainThread:2469618 [wandb_init.py:init():800] starting run threads in backend
2023-12-29 14:07:53,270 INFO    MainThread:2469618 [wandb_run.py:_console_start():2233] atexit reg
2023-12-29 14:07:53,270 INFO    MainThread:2469618 [wandb_run.py:_redirect():2088] redirect: wrap_raw
2023-12-29 14:07:53,270 INFO    MainThread:2469618 [wandb_run.py:_redirect():2153] Wrapping output streams.
2023-12-29 14:07:53,270 INFO    MainThread:2469618 [wandb_run.py:_redirect():2178] Redirects installed.
2023-12-29 14:07:53,270 INFO    MainThread:2469618 [wandb_init.py:init():841] run started, returning control to user process
2023-12-29 14:07:53,271 INFO    MainThread:2469618 [wandb_run.py:_config_callback():1342] config_cb None None {'param': 7213781, 'model': "PositionalUNet(\n  (inc): DoubleConv(\n    (double_conv): Sequential(\n      (0): Conv1d(1, 40, kernel_size=(11,), stride=(1,), padding=(5,), bias=False)\n      (1): BatchNorm1d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): LeakyReLU(negative_slope=0.01, inplace=True)\n      (3): Conv1d(40, 40, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n      (4): BatchNorm1d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): LeakyReLU(negative_slope=0.01, inplace=True)\n    )\n  )\n  (down1): Down(\n    (maxpool_conv): Sequential(\n      (0): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (1): DoubleConv(\n        (double_conv): Sequential(\n          (0): Conv1d(40, 80, kernel_size=(11,), stride=(1,), padding=(5,), bias=False)\n          (1): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): LeakyReLU(negative_slope=0.01, inplace=True)\n          (3): Conv1d(80, 80, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n          (4): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (5): LeakyReLU(negative_slope=0.01, inplace=True)\n        )\n      )\n    )\n  )\n  (down2): Down(\n    (maxpool_conv): Sequential(\n      (0): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (1): DoubleConv(\n        (double_conv): Sequential(\n          (0): Conv1d(80, 160, kernel_size=(11,), stride=(1,), padding=(5,), bias=False)\n          (1): BatchNorm1d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): LeakyReLU(negative_slope=0.01, inplace=True)\n          (3): Conv1d(160, 160, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n          (4): BatchNorm1d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (5): LeakyReLU(negative_slope=0.01, inplace=True)\n        )\n      )\n    )\n  )\n  (down3): Down(\n    (maxpool_conv): Sequential(\n      (0): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (1): DoubleConv(\n        (double_conv): Sequential(\n          (0): Conv1d(160, 320, kernel_size=(11,), stride=(1,), padding=(5,), bias=False)\n          (1): BatchNorm1d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): LeakyReLU(negative_slope=0.01, inplace=True)\n          (3): Conv1d(320, 320, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n          (4): BatchNorm1d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (5): LeakyReLU(negative_slope=0.01, inplace=True)\n        )\n      )\n    )\n  )\n  (down4): Down(\n    (maxpool_conv): Sequential(\n      (0): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (1): DoubleConv(\n        (double_conv): Sequential(\n          (0): Conv1d(320, 320, kernel_size=(11,), stride=(1,), padding=(5,), bias=False)\n          (1): BatchNorm1d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): LeakyReLU(negative_slope=0.01, inplace=True)\n          (3): Conv1d(320, 320, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n          (4): BatchNorm1d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (5): LeakyReLU(negative_slope=0.01, inplace=True)\n        )\n      )\n    )\n  )\n  (fc_mean): Conv1d(320, 320, kernel_size=(1,), stride=(1,))\n  (fc_var): Conv1d(320, 320, kernel_size=(1,), stride=(1,))\n  (up1): Up(\n    (up): Upsample(scale_factor=2.0, mode='linear')\n    (conv): DoubleConv(\n      (double_conv): Sequential(\n        (0): Conv1d(640, 320, kernel_size=(11,), stride=(1,), padding=(5,), bias=False)\n        (1): BatchNorm1d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): LeakyReLU(negative_slope=0.01, inplace=True)\n        (3): Conv1d(320, 160, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n        (4): BatchNorm1d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): LeakyReLU(negative_slope=0.01, inplace=True)\n      )\n    )\n  )\n  (up2): Up(\n    (up): Upsample(scale_factor=2.0, mode='linear')\n    (conv): DoubleConv(\n      (double_conv): Sequential(\n        (0): Conv1d(320, 160, kernel_size=(11,), stride=(1,), padding=(5,), bias=False)\n        (1): BatchNorm1d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): LeakyReLU(negative_slope=0.01, inplace=True)\n        (3): Conv1d(160, 80, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n        (4): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): LeakyReLU(negative_slope=0.01, inplace=True)\n      )\n    )\n  )\n  (up3): Up(\n    (up): Upsample(scale_factor=2.0, mode='linear')\n    (conv): DoubleConv(\n      (double_conv): Sequential(\n        (0): Conv1d(160, 80, kernel_size=(11,), stride=(1,), padding=(5,), bias=False)\n        (1): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): LeakyReLU(negative_slope=0.01, inplace=True)\n        (3): Conv1d(80, 40, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n        (4): BatchNorm1d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): LeakyReLU(negative_slope=0.01, inplace=True)\n      )\n    )\n  )\n  (up4): Up(\n    (up): Upsample(scale_factor=2.0, mode='linear')\n    (conv): DoubleConv(\n      (double_conv): Sequential(\n        (0): Conv1d(80, 40, kernel_size=(11,), stride=(1,), padding=(5,), bias=False)\n        (1): BatchNorm1d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): LeakyReLU(negative_slope=0.01, inplace=True)\n        (3): Conv1d(40, 20, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n        (4): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): LeakyReLU(negative_slope=0.01, inplace=True)\n      )\n    )\n  )\n  (outc): OutConv(\n    (conv): Sequential(\n      (0): Conv1d(20, 1, kernel_size=(1,), stride=(1,))\n    )\n  )\n  (pe1): PositionalEncoding(\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (pe2): PositionalEncoding(\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (pe3): PositionalEncoding(\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (pe4): PositionalEncoding(\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (pe5): PositionalEncoding(\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (pe6): PositionalEncoding(\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (pe7): PositionalEncoding(\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (pe8): PositionalEncoding(\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n)"}
2023-12-29 14:35:39,384 INFO    MainThread:2469618 [wandb_run.py:_finish():1962] finishing run CPU/26qmwwbw
2023-12-29 14:35:39,384 INFO    MainThread:2469618 [wandb_run.py:_atexit_cleanup():2202] got exitcode: 0
2023-12-29 14:35:39,385 INFO    MainThread:2469618 [wandb_run.py:_restore():2185] restore
2023-12-29 14:35:39,385 INFO    MainThread:2469618 [wandb_run.py:_restore():2191] restore done
2023-12-29 14:35:40,531 INFO    MainThread:2469618 [wandb_run.py:_footer_history_summary_info():3837] rendering history
2023-12-29 14:35:40,532 INFO    MainThread:2469618 [wandb_run.py:_footer_history_summary_info():3869] rendering summary

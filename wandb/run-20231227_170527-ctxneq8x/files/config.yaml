wandb_version: 1

_wandb:
  desc: null
  value:
    python_version: 3.8.8
    cli_version: 0.16.1
    framework: torch
    is_jupyter_run: false
    is_kaggle_kernel: false
    start_time: 1703725528.017707
    t:
      1:
      - 1
      - 5
      - 53
      - 55
      - 71
      2:
      - 1
      - 5
      - 53
      - 55
      - 71
      3:
      - 23
      4: 3.8.8
      5: 0.16.1
      8:
      - 5
      13: linux-x86_64
param:
  desc: null
  value: 7213781
model:
  desc: null
  value: "PositionalUNet(\n  (inc): DoubleConv(\n    (double_conv): Sequential(\n\
    \      (0): Conv1d(1, 40, kernel_size=(11,), stride=(1,), padding=(5,), bias=False)\n\
    \      (1): BatchNorm1d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n\
    \      (2): LeakyReLU(negative_slope=0.01, inplace=True)\n      (3): Conv1d(40,\
    \ 40, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n      (4): BatchNorm1d(40,\
    \ eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5):\
    \ LeakyReLU(negative_slope=0.01, inplace=True)\n    )\n  )\n  (down1): Down(\n\
    \    (maxpool_conv): Sequential(\n      (0): MaxPool1d(kernel_size=2, stride=2,\
    \ padding=0, dilation=1, ceil_mode=False)\n      (1): DoubleConv(\n        (double_conv):\
    \ Sequential(\n          (0): Conv1d(40, 80, kernel_size=(11,), stride=(1,), padding=(5,),\
    \ bias=False)\n          (1): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True,\
    \ track_running_stats=True)\n          (2): LeakyReLU(negative_slope=0.01, inplace=True)\n\
    \          (3): Conv1d(80, 80, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n\
    \          (4): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n\
    \          (5): LeakyReLU(negative_slope=0.01, inplace=True)\n        )\n    \
    \  )\n    )\n  )\n  (down2): Down(\n    (maxpool_conv): Sequential(\n      (0):\
    \ MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n\
    \      (1): DoubleConv(\n        (double_conv): Sequential(\n          (0): Conv1d(80,\
    \ 160, kernel_size=(11,), stride=(1,), padding=(5,), bias=False)\n          (1):\
    \ BatchNorm1d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n\
    \          (2): LeakyReLU(negative_slope=0.01, inplace=True)\n          (3): Conv1d(160,\
    \ 160, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n          (4):\
    \ BatchNorm1d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n\
    \          (5): LeakyReLU(negative_slope=0.01, inplace=True)\n        )\n    \
    \  )\n    )\n  )\n  (down3): Down(\n    (maxpool_conv): Sequential(\n      (0):\
    \ MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n\
    \      (1): DoubleConv(\n        (double_conv): Sequential(\n          (0): Conv1d(160,\
    \ 320, kernel_size=(11,), stride=(1,), padding=(5,), bias=False)\n          (1):\
    \ BatchNorm1d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n\
    \          (2): LeakyReLU(negative_slope=0.01, inplace=True)\n          (3): Conv1d(320,\
    \ 320, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n          (4):\
    \ BatchNorm1d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n\
    \          (5): LeakyReLU(negative_slope=0.01, inplace=True)\n        )\n    \
    \  )\n    )\n  )\n  (down4): Down(\n    (maxpool_conv): Sequential(\n      (0):\
    \ MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n\
    \      (1): DoubleConv(\n        (double_conv): Sequential(\n          (0): Conv1d(320,\
    \ 320, kernel_size=(11,), stride=(1,), padding=(5,), bias=False)\n          (1):\
    \ BatchNorm1d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n\
    \          (2): LeakyReLU(negative_slope=0.01, inplace=True)\n          (3): Conv1d(320,\
    \ 320, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n          (4):\
    \ BatchNorm1d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n\
    \          (5): LeakyReLU(negative_slope=0.01, inplace=True)\n        )\n    \
    \  )\n    )\n  )\n  (fc_mean): Conv1d(320, 320, kernel_size=(1,), stride=(1,))\n\
    \  (fc_var): Conv1d(320, 320, kernel_size=(1,), stride=(1,))\n  (up1): Up(\n \
    \   (up): Upsample(scale_factor=2.0, mode='linear')\n    (conv): DoubleConv(\n\
    \      (double_conv): Sequential(\n        (0): Conv1d(640, 320, kernel_size=(11,),\
    \ stride=(1,), padding=(5,), bias=False)\n        (1): BatchNorm1d(320, eps=1e-05,\
    \ momentum=0.1, affine=True, track_running_stats=True)\n        (2): LeakyReLU(negative_slope=0.01,\
    \ inplace=True)\n        (3): Conv1d(320, 160, kernel_size=(7,), stride=(1,),\
    \ padding=(3,), bias=False)\n        (4): BatchNorm1d(160, eps=1e-05, momentum=0.1,\
    \ affine=True, track_running_stats=True)\n        (5): LeakyReLU(negative_slope=0.01,\
    \ inplace=True)\n      )\n    )\n  )\n  (up2): Up(\n    (up): Upsample(scale_factor=2.0,\
    \ mode='linear')\n    (conv): DoubleConv(\n      (double_conv): Sequential(\n\
    \        (0): Conv1d(320, 160, kernel_size=(11,), stride=(1,), padding=(5,), bias=False)\n\
    \        (1): BatchNorm1d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n\
    \        (2): LeakyReLU(negative_slope=0.01, inplace=True)\n        (3): Conv1d(160,\
    \ 80, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n        (4): BatchNorm1d(80,\
    \ eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5):\
    \ LeakyReLU(negative_slope=0.01, inplace=True)\n      )\n    )\n  )\n  (up3):\
    \ Up(\n    (up): Upsample(scale_factor=2.0, mode='linear')\n    (conv): DoubleConv(\n\
    \      (double_conv): Sequential(\n        (0): Conv1d(160, 80, kernel_size=(11,),\
    \ stride=(1,), padding=(5,), bias=False)\n        (1): BatchNorm1d(80, eps=1e-05,\
    \ momentum=0.1, affine=True, track_running_stats=True)\n        (2): LeakyReLU(negative_slope=0.01,\
    \ inplace=True)\n        (3): Conv1d(80, 40, kernel_size=(7,), stride=(1,), padding=(3,),\
    \ bias=False)\n        (4): BatchNorm1d(40, eps=1e-05, momentum=0.1, affine=True,\
    \ track_running_stats=True)\n        (5): LeakyReLU(negative_slope=0.01, inplace=True)\n\
    \      )\n    )\n  )\n  (up4): Up(\n    (up): Upsample(scale_factor=2.0, mode='linear')\n\
    \    (conv): DoubleConv(\n      (double_conv): Sequential(\n        (0): Conv1d(80,\
    \ 40, kernel_size=(11,), stride=(1,), padding=(5,), bias=False)\n        (1):\
    \ BatchNorm1d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n\
    \        (2): LeakyReLU(negative_slope=0.01, inplace=True)\n        (3): Conv1d(40,\
    \ 20, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n        (4): BatchNorm1d(20,\
    \ eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5):\
    \ LeakyReLU(negative_slope=0.01, inplace=True)\n      )\n    )\n  )\n  (outc):\
    \ OutConv(\n    (conv): Sequential(\n      (0): Conv1d(20, 1, kernel_size=(1,),\
    \ stride=(1,))\n    )\n  )\n  (pe1): PositionalEncoding(\n    (dropout): Dropout(p=0.1,\
    \ inplace=False)\n  )\n  (pe2): PositionalEncoding(\n    (dropout): Dropout(p=0.1,\
    \ inplace=False)\n  )\n  (pe3): PositionalEncoding(\n    (dropout): Dropout(p=0.1,\
    \ inplace=False)\n  )\n  (pe4): PositionalEncoding(\n    (dropout): Dropout(p=0.1,\
    \ inplace=False)\n  )\n  (pe5): PositionalEncoding(\n    (dropout): Dropout(p=0.1,\
    \ inplace=False)\n  )\n  (pe6): PositionalEncoding(\n    (dropout): Dropout(p=0.1,\
    \ inplace=False)\n  )\n  (pe7): PositionalEncoding(\n    (dropout): Dropout(p=0.1,\
    \ inplace=False)\n  )\n  (pe8): PositionalEncoding(\n    (dropout): Dropout(p=0.1,\
    \ inplace=False)\n  )\n)"
